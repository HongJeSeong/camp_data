{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8nTy7_MxmmR"
      },
      "source": [
        "!rm -rf camp_data\r\n",
        "!git clone https://github.com/HongJeSeong/camp_data.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zttfHdSxo_X"
      },
      "source": [
        "!ls camp_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKaScMsMxsT9"
      },
      "source": [
        "import pandas as pd\r\n",
        "df = pd.read_csv(\"camp_data/review.txt\", names=['sentence', 'label'], sep='\\t')\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7SRlPJ60lSW"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "x = df['sentence'].values\r\n",
        "y = df['label'].values\r\n",
        "\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9KDQlHV9fZX"
      },
      "source": [
        "`tokenizer = Tokenizer(num_words=500)`  \r\n",
        "\r\n",
        "데이터를 읽어 들이고\r\n",
        "단어 빈도에 따른 사용할 단어 개수의 최대값. 가장 빈번하게 사용되는 num_words개의 단어만 저장  \r\n",
        "가장 많이 출현한 단어 Top 500을 선택한다는 의미\r\n",
        "\r\n",
        "`xtrain= tokenizer.texts_to_sequences(x_train)`\r\n",
        "\r\n",
        "Top 500개로 저장한 단어사전에서 매칭되는 단어들을 선택하여 문장 형식으로 변경\r\n",
        "\r\n",
        "![](https://raw.githubusercontent.com/HongJeSeong/camp_data/main/img/wordindex.PNG)\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9fmcW8sxy7t"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words=500)\r\n",
        "tokenizer.fit_on_texts(x)\r\n",
        "xtrain= tokenizer.texts_to_sequences(x_train)\r\n",
        "xtest= tokenizer.texts_to_sequences(x_test) \r\n",
        "vocab_size=len(tokenizer.word_index)+1 # 단어들이 저장된 단어 사전의 크기 + 1(공백단어때문에 +1)\r\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwJcn5VY6WEQ"
      },
      "source": [
        "tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jm207W1x6gF"
      },
      "source": [
        "x_train[5], xtrain[5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqDAsHxvAnCW"
      },
      "source": [
        "# Padding\r\n",
        "![padding](https://raw.githubusercontent.com/HongJeSeong/camp_data/main/img/padding.PNG)\r\n",
        "(https://raw.githubusercontent.com/HongJeSeong/camp_data/main/img/padding.PNG)\r\n",
        "\r\n",
        "원래의 입력 단어가 SIZE 10을 `넘는다면` ?  \r\n",
        "10까지만 넣어지고 나머지 뒤 부분은 잘림\r\n",
        "\r\n",
        "## post, pre\r\n",
        "\r\n",
        "![](https://raw.githubusercontent.com/HongJeSeong/camp_data/main/img/prepost.PNG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QADXxUAU20GO"
      },
      "source": [
        "xtrain=pad_sequences(xtrain,padding='post', maxlen=20)\r\n",
        "xtest=pad_sequences(xtest,padding='post', maxlen=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF4aw6ID3ayl"
      },
      "source": [
        "xtrain[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6osqav0P3cFd"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import LSTM,Dense, Dropout\r\n",
        "from tensorflow.keras.layers import Embedding\r\n",
        "embedding_vector_length = 32\r\n",
        "model = Sequential()\r\n",
        "model.add(Embedding(vocab_size, embedding_vector_length, input_length=20))\r\n",
        "model.add(LSTM(50, dropout=0.5)\r\n",
        "model.add(Dropout(0.4))\r\n",
        "model.add(Dense(1, activation='sigmoid'))\r\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsJNYm5tPVaD"
      },
      "source": [
        "### Embedding\r\n",
        "`Embedding(vocab_size, embedding_vector_length, input_length=20)`\r\n",
        "* voca_size(input_dim) : 단어 사전의 크기를, 총 5272개의 단어 종류가 있다는 뜻\r\n",
        "* embedding_vector_length(output_dim) : 단어를 인코딩 한 후 나오는 벡터 크기 의미론적 기하공간에 나타낸다는 의미  \r\n",
        "즉 임베딩 레이어는 입력되는 단어를 의미론적으로 잘 설계된 공간에 위치시켜 벡터로 수치화 시킨다\r\n",
        "* input_length(input_length) : 단어의 수 즉, 문장의 길이를 의미 \r\n",
        "임베딩 레이어 다음에 Flatten 레이어가 온다면 반드시 input_lenth를 지정해야 함 \r\n",
        "Flatten 레이어인 경우 입력 크기가 알아야 이를 1차원으로 만들어서 Dense 레이어에 전달할 수 있기 때문"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnYazN4vBKcB"
      },
      "source": [
        "![d](https://raw.githubusercontent.com/HongJeSeong/camp_data/main/img/dropout.PNG)\r\n",
        "(https://raw.githubusercontent.com/HongJeSeong/camp_data/main/img/dropout.PNG)\r\n",
        "학습의 오버피팅을 줄이기 위한 레이어"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2X4p8oLg4evN",
        "collapsed": true
      },
      "source": [
        "history = model.fit(xtrain,y_train, epochs=120, verbose=1,validation_data=(xtest,y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xv4LLjIbo_d5"
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "plt.figure(figsize=(18, 10))\r\n",
        "plt.plot(history.history['accuracy'])\r\n",
        "plt.plot(history.history['val_accuracy'])\r\n",
        "plt.title('model_accuracy')\r\n",
        "plt.ylabel('accuracy')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.legend(['train', 'test'], loc='best')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h61ecf3gqh3r"
      },
      "source": [
        "import numpy as np\r\n",
        "def pred_result(text,model):\r\n",
        "  text= tokenizer.texts_to_sequences(text)\r\n",
        "  text=pad_sequences(text,padding='post', maxlen=20)\r\n",
        "  pred = model.predict(text)\r\n",
        "\r\n",
        "  print(\"확률 : \",pred)\r\n",
        "  if pred > 0.5:\r\n",
        "    print(\"Positive\")\r\n",
        "  else:\r\n",
        "    print(\"Negative\")\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtokpXxRvkxI"
      },
      "source": [
        "pred_result([\"great\"],model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqTe79lEyFgi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}